# MoDA: Multi-modal Diffusion Architecture for Talking Head Generation

**Authors:** Xinyang LiÂ¹Â², Gen LiÂ², Zhihui LinÂ¹Â³, Yichen QianÂ¹Â³ â€ , Gongxin YaoÂ², Weinan JiaÂ¹, Weihua ChenÂ¹Â³, Fan WangÂ¹Â³  
Â¹Xunguang Team, DAMO Academy, Alibaba Group  
Â²Zhejiang University  
Â³Hupan Lab  
â€ Corresponding author: yichen.qyc@alibaba-inc.com, l_xyang@zju.edu.cn

---

## ğŸ“– Overview

MoDA introduces a novel diffusionâ€based framework that jointly models motion generation and neural rendering to produce realistic talking-head videos from a single portrait and arbitrary audio inputs. Key contributions include:  
1. **Joint Parameter Space:** Bridges motion generation and neural rendering via a disentangled motionâ€appearance representation, integrating audio, emotion, and identity conditions.  
2. **Multi-modal Diffusion Architecture:** Enables deep interaction among noisy motion trajectories, speech audio, and auxiliary signals, significantly enhancing facial expressiveness and lipâ€“sync accuracy. :contentReference[oaicite:0]{index=0}

---

## ğŸš€ Demo

Try out the live demo here:  
[â–¶ï¸ Live MoDA Demo](https://your-demo-link.com)

---

## ğŸ“‚ Code

The full implementation will be released soon. Stay tuned!  

---

## ğŸ“„ Paper

Read the full preprint on arXiv:  
[MoDA: Multi-modal Diffusion Architecture for Talking Head Generation](https://arxiv.org/abs/2507.03256) :contentReference[oaicite:1]{index=1}

---

## ğŸ“‘ Citation

If you use MoDA in your research, please cite:

```bibtex
@article{li2025moda,
  title     = {MoDA: Multi-modal Diffusion Architecture for Talking Head Generation},
  author    = {Li, Xinyang and Li, Gen and Lin, Zhihui and Qian, Yichen and Yao, Gongxin and Jia, Weinan and Chen, Weihua and Wang, Fan},
  journal   = {arXiv preprint arXiv:2507.03256},
  year      = {2025},
}
