# MoDA: Multi-modal Diffusion Architecture for Talking Head Generation

**Authors:** Xinyang Li¹², Gen Li², Zhihui Lin¹³, Yichen Qian¹³ †, Gongxin Yao², Weinan Jia¹, Weihua Chen¹³, Fan Wang¹³  
¹Xunguang Team, DAMO Academy, Alibaba Group  
²Zhejiang University  
³Hupan Lab  
†Corresponding author: yichen.qyc@alibaba-inc.com, l_xyang@zju.edu.cn

---

## 📖 Overview

MoDA introduces a novel diffusion‐based framework that jointly models motion generation and neural rendering to produce realistic talking-head videos from a single portrait and arbitrary audio inputs. Key contributions include:  
1. **Joint Parameter Space:** Bridges motion generation and neural rendering via a disentangled motion‐appearance representation, integrating audio, emotion, and identity conditions.  
2. **Multi-modal Diffusion Architecture:** Enables deep interaction among noisy motion trajectories, speech audio, and auxiliary signals, significantly enhancing facial expressiveness and lip–sync accuracy. :contentReference[oaicite:0]{index=0}

---

## 🚀 Demo

Try out the live demo here:  
[▶️ Live MoDA Demo](https://your-demo-link.com)

---

## 📂 Code

The full implementation will be released soon. Stay tuned!  

---

## 📄 Paper

Read the full preprint on arXiv:  
[MoDA: Multi-modal Diffusion Architecture for Talking Head Generation](https://arxiv.org/abs/2507.03256) :contentReference[oaicite:1]{index=1}

---

## 📑 Citation

If you use MoDA in your research, please cite:

```bibtex
@article{li2025moda,
  title     = {MoDA: Multi-modal Diffusion Architecture for Talking Head Generation},
  author    = {Li, Xinyang and Li, Gen and Lin, Zhihui and Qian, Yichen and Yao, Gongxin and Jia, Weinan and Chen, Weihua and Wang, Fan},
  journal   = {arXiv preprint arXiv:2507.03256},
  year      = {2025},
}
