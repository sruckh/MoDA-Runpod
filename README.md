# MoDA: Multi-modal Diffusion Architecture for Talking-Head Generation

MoDA is a unified framework that fuses audio, identity, emotion, and motion cues within a single diffusion process to synthesize photorealistic, expressive talking-head videos.

<p align="center">
  <img src="assets/moda_teaser.gif" width="720">
</p>

---

## Paper
**MoDA: Multi-modal Diffusion Architecture for Talking-Head Generation**  
• arXiv preprint: <https://arxiv.org/abs/XXXX.XXXXX>  
• Project / demo page: <https://your-demo-link.com>

---

## Demo

Try MoDA online: **<https://your-demo-link.com>**

Upload a reference frame and an audio clip—MoDA will return a high-fidelity talking-head video in a few seconds.

---

## Code ‑ Coming Soon

We are finalizing the source code, trained checkpoints, and evaluation scripts.  
Please **star** ⭐ or **watch** 🔔 this repository to get notified when they are released.

```text
📅 ETA: Q4 2024
