# MoDA: Multi-modal Diffusion Architecture for Talking-Head Generation

MoDA is a unified framework that fuses audio, identity, emotion, and motion cues within a single diffusion process to synthesize photorealistic, expressive talking-head videos.

<p align="center">
  <img src="assets/moda_teaser.gif" width="720">
</p>

---

## Paper
**MoDA: Multi-modal Diffusion Architecture for Talking-Head Generation**  
â€¢ arXiv preprint: <https://arxiv.org/abs/XXXX.XXXXX>  
â€¢ Project / demo page: <https://your-demo-link.com>

---

## Demo

Try MoDA online: **<https://your-demo-link.com>**

Upload a reference frame and an audio clipâ€”MoDA will return a high-fidelity talking-head video in a few seconds.

---

## Code â€‘ Coming Soon

We are finalizing the source code, trained checkpoints, and evaluation scripts.  
Please **star** â­ or **watch** ğŸ”” this repository to get notified when they are released.

```text
ğŸ“… ETA: Q4 2024
